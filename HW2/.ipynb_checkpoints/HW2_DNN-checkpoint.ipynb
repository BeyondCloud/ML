{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "fold: 0\n",
      "epoch: 0\n",
      "label \t accuracy \t precision \t recall\n",
      "0 \t 0.99 \t\t 0.964 \t\t 0.983\n",
      "1 \t 0.983 \t\t 0.944 \t\t 0.983\n",
      "2 \t 0.968 \t\t 0.948 \t\t 0.882\n",
      "3 \t 0.973 \t\t 0.927 \t\t 0.941\n",
      "4 \t 0.988 \t\t 0.975 \t\t 0.961\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-33e38b83e4cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m                 \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_total\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_total\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m                 \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "######################\n",
    "#Brief:\n",
    "# 5 layer DNN ,128 neurons per layer\n",
    "# implement:\n",
    "#  batch input(for GPU optimize)\n",
    "#  N-fold cross validation\n",
    "#  print accuacy,precision,recall\n",
    "#  Dropout\n",
    "#  Optimize:\n",
    "#  batch_norm,specify momentum parameter of Adamoptimizer\n",
    "#input:\n",
    "# mnist hand writing dataset 28*28 image(0~5 only)\n",
    "# output: 0~5 predicted number\n",
    "# Result:\n",
    "# We found that\n",
    "####################\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from tensorflow.contrib.layers import fully_connected, batch_norm\n",
    "\n",
    "###### Do not modify here ###### \n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "# training on MNIST but only on digits 0 to 4\n",
    "X_train1 = mnist.train.images[mnist.train.labels < 5] #(28038,784)\n",
    "y_train1 = mnist.train.labels[mnist.train.labels < 5] #(28038,)\n",
    "\n",
    "X_valid1 = mnist.validation.images[mnist.validation.labels < 5] #(2558,784)\n",
    "y_valid1 = mnist.validation.labels[mnist.validation.labels < 5] #(2558,) \n",
    "\n",
    "X_test1 = mnist.test.images[mnist.test.labels < 5] #(5139,784)\n",
    "y_test1 = mnist.test.labels[mnist.test.labels < 5] #(5139,)\n",
    "\n",
    "###### Do not modify here ###### \n",
    "\n",
    "#get next batch in order\n",
    "def next_batch(batch_size,iteration, data, labels):\n",
    "    start = batch_size*iteration\n",
    "    end = batch_size*(iteration+1)\n",
    "    return data[start:end], labels[start:end]\n",
    "\n",
    "#const parameters\n",
    "n_inputs = 784  # MNIST\n",
    "n_outputs = 5\n",
    "\n",
    "#adjustable parameters\n",
    "N_neurons = 128\n",
    "learning_rate = 0.01\n",
    "momentum = 0.25\n",
    "epochs = 2\n",
    "batch_size = 128  #for GPU optimize\n",
    "N_fold = 3\n",
    "dropout = 0.5\n",
    "\n",
    "# Create the model\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "#improving traning speed\n",
    "batch_norm_params = {\n",
    "    'is_training': is_training,\n",
    "    'decay': 0.9,\n",
    "    'updates_collections': None,\n",
    "    'scale': True,\n",
    "}\n",
    "\n",
    "\n",
    "#5 fully connected layer ,128 neurons per layer with dropout\n",
    "with arg_scope(\n",
    "        [fully_connected],\n",
    "        activation_fn=tf.nn.elu,\n",
    "        weights_initializer=he_init,\n",
    "        normalizer_fn=batch_norm,\n",
    "        normalizer_params=batch_norm_params):\n",
    "    W1 = fully_connected(X,N_neurons)\n",
    "    W1_D = tf.nn.dropout(W1,dropout)\n",
    "    W2 = fully_connected(W1_D,N_neurons)\n",
    "    W2_D = tf.nn.dropout(W2,dropout)\n",
    "    W3 = fully_connected(W2_D,N_neurons)\n",
    "    W3_D = tf.nn.dropout(W3,dropout)\n",
    "    W4 = fully_connected(W3,N_neurons)\n",
    "    W4_D = tf.nn.dropout(W4,dropout)\n",
    "    W5 = fully_connected(W4_D,N_neurons)\n",
    "    W5_D = tf.nn.dropout(W5,dropout)\n",
    "    y_hat = fully_connected(W5_D, n_outputs, activation_fn=None)\n",
    "\n",
    "\n",
    "#Add softmax to output\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=y_hat)\n",
    "loss = tf.reduce_mean(cross_entropy, name=\"loss\")\n",
    "\n",
    "#use AdamOptimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, momentum)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "#use for calculate TP TN FP FN\n",
    "y_hat_argmax = tf.argmax(y_hat,1) \n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#START N-fold version\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "k_fold = KFold(n_splits=N_fold)\n",
    "X_total = np.concatenate((X_train1 ,X_valid1),axis = 0)\n",
    "y_total = np.concatenate((y_train1 ,y_valid1),axis = 0)\n",
    "fold_idx = 0\n",
    "\n",
    "\n",
    "#parameters for caculating accuracy,precision and recall\n",
    "ACC = np.zeros([N_fold,5])\n",
    "PRE = np.zeros([N_fold,5])\n",
    "REC = np.zeros([N_fold,5])\n",
    "TP = np.zeros(5)\n",
    "TN = np.zeros(5)\n",
    "FP = np.zeros(5)\n",
    "FN = np.zeros(5)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for train_index, valid_index in k_fold.split(X_total):\n",
    "        init.run()\n",
    "        print(\"fold:\",fold_idx)\n",
    "        for epoch in range(epochs):\n",
    "            for iteration in range(len(train_index)//batch_size):\n",
    "                X_batch, y_batch = next_batch(batch_size,iteration,X_total[train_index], y_total[train_index])\n",
    "                sess.run(train_op, feed_dict={is_training: True, X: X_batch, y: y_batch})\n",
    "            \n",
    "            print(\"epoch:\",epoch)\n",
    "            y_predict = y_hat_argmax.eval(feed_dict={is_training: False, X: X_total[valid_index], y: y_total[valid_index]})\n",
    "            TP.fill(0)\n",
    "            TN.fill(0)\n",
    "            FP.fill(0)\n",
    "            FN.fill(0)\n",
    "            for i in range(len(y_predict)):\n",
    "                #the if else can be simplified , but I prefer this since it's more readable\n",
    "                #if predict right \n",
    "                #increment TP[predict_y] with 1\n",
    "                #increment TN[other] with 1\n",
    "                #if wrong\n",
    "                #increment FP[predict_y] with 1\n",
    "                #increment FN[y] with 1\n",
    "                #increment FN[y] with 1\n",
    "                #increment TN[other] with 1\n",
    "                if y_predict[i] ==  y_total[valid_index[i]]:\n",
    "                    TP[y_predict[i]]+=1\n",
    "                    TN+=1\n",
    "                    TN[y_predict[i]]-=1\n",
    "                else:\n",
    "                    FP[y_predict[i]]+=1\n",
    "                    FN[y_total[i]]+=1\n",
    "                    TN+=1\n",
    "                    TN[y_predict[i]]-=1\n",
    "                    TN[y_total[i]]-=1\n",
    "            print(\"label\",\"\\t\",\"accuracy\",\"\\t\",\"precision\",\"\\t\",\"recall\")\n",
    "            for i in range(5):\n",
    "                ACC[fold_idx,i] = round((TP[i]+TN[i])/(TP[i]+TN[i]+FP[i]+FN[i]),3)\n",
    "                PRE[fold_idx,i] = round((TP[i])/(TP[i]+FP[i]),3)\n",
    "                REC[fold_idx,i] = round((TP[i])/(TP[i]+FN[i]),3)\n",
    "                print(i,\"\\t\",ACC[fold_idx,i],\"\\t\\t\",PRE[fold_idx,i],\"\\t\\t\",REC[fold_idx,i])\n",
    "        fold_idx+=1\n",
    "         \n",
    "    print(\"average result:\")\n",
    "    print(\"label\",\"\\t\",\"accuracy\",\"\\t\",\"precision\",\"\\t\",\"recall\")\n",
    "    for i in range(5):\n",
    "        print(i,\"\\t\",np.mean(ACC,0)[i],\"\\t\\t\",np.mean(PRE,0)[i],\"\\t\\t\",np.mean(REC,0)[i])\n",
    "    save_path = saver.save(sess, \"./model/Team11_HW2.ckpt\")\n",
    "\n",
    "#END N-fold version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
