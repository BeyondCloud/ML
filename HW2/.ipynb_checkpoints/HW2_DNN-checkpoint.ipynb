{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "fold: 0\n",
      "epoch: 0\n",
      "label \t accuracy \t precision \t recall\n",
      "1958.0   8196.0   19.0   26.0\n",
      "0 \t 0.996 \t\t 0.99 \t\t 0.987\n",
      "2230.0   7905.0   24.0   40.0\n",
      "1 \t 0.994 \t\t 0.989 \t\t 0.982\n",
      "1877.0   8152.0   93.0   77.0\n",
      "2 \t 0.983 \t\t 0.953 \t\t 0.961\n",
      "2025.0   8039.0   69.0   66.0\n",
      "3 \t 0.987 \t\t 0.967 \t\t 0.968\n",
      "1871.0   8266.0   33.0   29.0\n",
      "4 \t 0.994 \t\t 0.983 \t\t 0.985\n",
      "epoch: 1\n",
      "label \t accuracy \t precision \t recall\n",
      "1970.0   8195.0   20.0   14.0\n",
      "0 \t 0.997 \t\t 0.99 \t\t 0.993\n",
      "2238.0   7905.0   24.0   32.0\n",
      "1 \t 0.995 \t\t 0.989 \t\t 0.986\n",
      "1908.0   8180.0   65.0   46.0\n",
      "2 \t 0.989 \t\t 0.967 \t\t 0.976\n",
      "2036.0   8071.0   37.0   55.0\n",
      "3 \t 0.991 \t\t 0.982 \t\t 0.974\n",
      "1877.0   8275.0   24.0   23.0\n",
      "4 \t 0.995 \t\t 0.987 \t\t 0.988\n",
      "fold: 1\n",
      "epoch: 0\n",
      "label \t accuracy \t precision \t recall\n",
      "384.0   6625.0   1590.0   1600.0\n",
      "0 \t 0.687 \t\t 0.195 \t\t 0.194\n",
      "507.0   6145.0   1784.0   1763.0\n",
      "1 \t 0.652 \t\t 0.221 \t\t 0.223\n",
      "390.0   6616.0   1629.0   1564.0\n",
      "2 \t 0.687 \t\t 0.193 \t\t 0.2\n",
      "412.0   6547.0   1561.0   1679.0\n",
      "3 \t 0.682 \t\t 0.209 \t\t 0.197\n",
      "387.0   6744.0   1555.0   1513.0\n",
      "4 \t 0.699 \t\t 0.199 \t\t 0.204\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-385f7a0d3884>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m                 \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_total\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_total\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m                 \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"epoch:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from tensorflow.contrib.layers import fully_connected, batch_norm\n",
    "\n",
    "###### Do not modify here ###### \n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "# training on MNIST but only on digits 0 to 4\n",
    "X_train1 = mnist.train.images[mnist.train.labels < 5] #(28038,784)\n",
    "y_train1 = mnist.train.labels[mnist.train.labels < 5] #(28038,)\n",
    "\n",
    "X_valid1 = mnist.validation.images[mnist.validation.labels < 5] #(2558,784)\n",
    "y_valid1 = mnist.validation.labels[mnist.validation.labels < 5] #(2558,) \n",
    "\n",
    "X_test1 = mnist.test.images[mnist.test.labels < 5] #(5139,784)\n",
    "y_test1 = mnist.test.labels[mnist.test.labels < 5] #(5139,)\n",
    "\n",
    "###### Do not modify here ###### \n",
    "\n",
    "#get next batch in order\n",
    "def next_batch(num,iteration, data, labels):\n",
    "    start = num*iteration\n",
    "    end = num*(iteration+1)\n",
    "    return data[start:end], labels[start:end]\n",
    "\n",
    "#get next batch by random pick nums of data\n",
    "def next_batch_rnd(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    #get n random index\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    ####\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "\n",
    "# Specify that all features have real-value data\n",
    "\n",
    "# Create the model\n",
    "n_inputs = 784  # MNIST\n",
    "N_neurons = 128\n",
    "n_outputs = 5\n",
    "learning_rate = 0.01\n",
    "momentum = 0.25\n",
    "epochs = 2\n",
    "batch_size = 128\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "#improving traning speed\n",
    "batch_norm_params = {\n",
    "    'is_training': is_training,\n",
    "    'decay': 0.9,\n",
    "    'updates_collections': None,\n",
    "    'scale': True,\n",
    "}\n",
    "\n",
    "#arg_scope: apply arg to all command below\n",
    "with arg_scope(\n",
    "        [fully_connected],\n",
    "        activation_fn=tf.nn.elu,\n",
    "        weights_initializer=he_init,\n",
    "        normalizer_fn=batch_norm,\n",
    "        normalizer_params=batch_norm_params):\n",
    "    W1 = fully_connected(X,N_neurons)\n",
    "    W2 = fully_connected(W1,N_neurons)\n",
    "    y_hat = fully_connected(W2, n_outputs, activation_fn=None)\n",
    "#     W3 = fully_connected(W2,N_neurons)\n",
    "#     W4 = fully_connected(W3,N_neurons)\n",
    "#     W5 = fully_connected(W4,N_neurons)\n",
    "#     y_hat = fully_connected(W5, n_outputs, activation_fn=None)\n",
    "\n",
    "#Add softmax to output\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=y_hat)\n",
    "loss = tf.reduce_mean(cross_entropy, name=\"loss\")\n",
    "\n",
    "#use AdamOptimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, momentum)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "y_hat_argmax = tf.argmax(y_hat,1) \n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#START no N-fold version\n",
    "# with tf.Session() as sess:\n",
    "#     init.run()\n",
    "#     for epoch in range(epochs):\n",
    "#         for iteration in range(len(X_train1)//batch_size):\n",
    "#             X_batch, y_batch = next_batch(batch_size,iteration,X_train1, y_train1)\n",
    "#             sess.run(train_op, feed_dict={is_training: True, X: X_batch, y: y_batch})\n",
    "#         acc_train = acc.eval(feed_dict={is_training: False, X: X_batch, y: y_batch})\n",
    "#         acc_valid = acy.eval(feed_dict={is_training: False, X:  X_valid1, y:  y_valid1})\n",
    "#         print(epoch, \"Train accuracy:\", acc_train, \"Valid accuracy:\", acc_valid)\n",
    "#     save_path = saver.save(sess, \"./model/Team11_HW2.ckpt\")\n",
    "#     acc_test = acc.eval(feed_dict={is_training: False, X:  X_test1, y:  y_test1})\n",
    "#     print( \"Test accuracy:\", acc_test)\n",
    "#END no N-fold version \n",
    "\n",
    "#START N-fold version\n",
    "from sklearn.model_selection import KFold\n",
    "N_fold = 3\n",
    "X_total = np.concatenate((X_train1 ,X_valid1),axis = 0)\n",
    "y_total = np.concatenate((y_train1 ,y_valid1),axis = 0)\n",
    "fold_idx = 0\n",
    "ACC = np.zeros([N_fold,5])\n",
    "PRE = np.zeros([N_fold,5])\n",
    "REC = np.zeros([N_fold,5])\n",
    "TP = np.zeros(5)\n",
    "TN = np.zeros(5)\n",
    "FP = np.zeros(5)\n",
    "FN = np.zeros(5)\n",
    "with tf.Session() as sess:\n",
    "    for train_index, valid_index in k_fold.split(X_total):\n",
    "        init.run()\n",
    "        print(\"fold:\",fold_idx)\n",
    "        for epoch in range(epochs):\n",
    "            for iteration in range(len(train_index)//batch_size):\n",
    "                X_batch, y_batch = next_batch(batch_size,iteration,X_total[train_index], y_total[train_index])\n",
    "                sess.run(train_op, feed_dict={is_training: True, X: X_batch, y: y_batch})\n",
    "            \n",
    "            print(\"epoch:\",epoch)\n",
    "            y_predict = y_hat_argmax.eval(feed_dict={is_training: False, X: X_total[valid_index], y: y_total[valid_index]})\n",
    "            TP.fill(0)\n",
    "            TN.fill(0)\n",
    "            FP.fill(0)\n",
    "            FN.fill(0)\n",
    "            for i in range(len(y_predict)):\n",
    "                #these if else can be simplified , but I prefer this since it's more readable\n",
    "                if y_predict[i] ==  y_total[i]:\n",
    "                    TP[y_predict[i]]+=1\n",
    "                    TN+=1\n",
    "                    TN[y_predict[i]]-=1\n",
    "                else:\n",
    "                    FP[y_predict[i]]+=1\n",
    "                    FN[y_total[i]]+=1\n",
    "                    TN+=1\n",
    "                    TN[y_predict[i]]-=1\n",
    "                    TN[y_total[i]]-=1\n",
    "            print(\"label\",\"\\t\",\"accuracy\",\"\\t\",\"precision\",\"\\t\",\"recall\")\n",
    "            \n",
    "            for i in range(5):\n",
    "                print(TP[i],\" \",TN[i],\" \",FP[i],\" \",FN[i])\n",
    "                ACC[fold_idx,i] = round((TP[i]+TN[i])/(TP[i]+TN[i]+FP[i]+FN[i]),3)\n",
    "                PRE[fold_idx,i] = round((TP[i])/(TP[i]+FP[i]),3)\n",
    "                REC[fold_idx,i] = round((TP[i])/(TP[i]+FN[i]),3)\n",
    "                print(i,\"\\t\",ACC[fold_idx][i],\"\\t\\t\",PRE[fold_idx][i],\"\\t\\t\",REC[fold_idx][i])\n",
    "        fold_idx+=1\n",
    "         \n",
    "    print(\"average result:\")\n",
    "    print(\"label\",\"\\t\",\"accuracy\",\"\\t\",\"precision\",\"\\t\",\"recall\")\n",
    "    for i in range(5):\n",
    "        print(i,\"\\t\",np.mean(ACC,0)[i],\"\\t\\t\",np.mean(PRE,0)[i],\"\\t\\t\",np.mean(REC,0)[i])\n",
    "    save_path = saver.save(sess, \"./model/Team11_HW2.ckpt\")\n",
    "\n",
    "#END N-fold version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  4.]\n"
     ]
    }
   ],
   "source": [
    "foo = np.array([[1,1],[2,3],[3,8]])\n",
    "test  = np.mean(foo,0)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [2 3 4 5] | test: [0 1]\n",
      "Train: [0 1 4 5] | test: [2 3]\n",
      "Train: [0 1 2 3] | test: [4 5]\n",
      "[ 0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "X = [\"a\", \"a\", \"b\", \"c\", \"c\", \"c\"]\n",
    "k_fold = KFold(n_splits=3)\n",
    "for train_indices, test_indices in k_fold.split(X):\n",
    "    print('Train: %s | test: %s' % (train_indices, test_indices))\n",
    "a = np.array([[1,2,3],[4,5,6]])\n",
    "b = np.array([[1,2,3],[4,5,6]])\n",
    "c = np.concatenate((X_train1 ,X_valid1),axis = 0)\n",
    "foo = np.zeros(3)\n",
    "print(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "Activation Functions\n",
    "\n",
    "def logit(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "plt.show()\n",
    "\n",
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)\n",
    "\n",
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "plt.show()\n",
    "\n",
    "def elu(z, alpha=1):\n",
    "    return np.where(z<0, alpha*(np.exp(z)-1), z)\n",
    "\n",
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "\n",
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01*z, z, name=name)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28*28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = fully_connected(X, n_hidden1, activation_fn=leaky_relu, scope=\"hidden1\")\n",
    "    hidden2 = fully_connected(hidden1, n_hidden2, activation_fn=leaky_relu, scope=\"hidden2\")\n",
    "    logits = fully_connected(hidden2, n_outputs, activation_fn=None, scope=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "   \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(len(mnist.test.labels)//batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"my_model_final.ckpt\")\n",
    "\n",
    "Batch Normalization\n",
    "\n",
    "from tensorflow.contrib.layers import fully_connected, batch_norm\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "learning_rate = 0.01\n",
    "momentum = 0.25\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    batch_norm_params = {\n",
    "        'is_training': is_training,\n",
    "        'decay': 0.9,\n",
    "        'updates_collections': None,\n",
    "        'scale': True,\n",
    "    }\n",
    "\n",
    "    with arg_scope(\n",
    "            [fully_connected],\n",
    "            activation_fn=tf.nn.elu,\n",
    "            weights_initializer=he_init,\n",
    "            normalizer_fn=batch_norm,\n",
    "            normalizer_params=batch_norm_params):\n",
    "        hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\")\n",
    "        hidden2 = fully_connected(hidden1, n_hidden2, scope=\"hidden2\")\n",
    "        logits = fully_connected(hidden2, n_outputs, activation_fn=None, scope=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "   \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(len(mnist.test.labels)//batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={is_training: True, X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={is_training: False, X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={is_training: False, X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "save_path = saver.save(sess, \"my_model_final.ckpt\")\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    batch_norm_params = {\n",
    "        'is_training': is_training,\n",
    "        'decay': 0.9,\n",
    "        'updates_collections': None,\n",
    "        'scale': True,\n",
    "    }\n",
    "\n",
    "    with arg_scope(\n",
    "            [fully_connected],\n",
    "            activation_fn=tf.nn.elu,\n",
    "            weights_initializer=he_init,\n",
    "            normalizer_fn=batch_norm,\n",
    "            normalizer_params=batch_norm_params,\n",
    "            weights_regularizer=tf.contrib.layers.l1_regularizer(0.01)):\n",
    "        hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\")\n",
    "        hidden2 = fully_connected(hidden1, n_hidden2, scope=\"hidden2\")\n",
    "        logits = fully_connected(hidden2, n_outputs, activation_fn=None, scope=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"base_loss\")\n",
    "    loss = tf.add(base_loss, reg_losses, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "Gradient Clipping\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(len(mnist.test.labels)//batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={is_training: True, X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={is_training: False, X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={is_training: False, X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"my_model_final.ckpt\")\n",
    "\n",
    "[v.name for v in tf.global_variables()]\n",
    "\n",
    "\n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):  # root scope\n",
    "    weights1 = tf.get_variable(\"hidden1/weights\")\n",
    "    weights2 = tf.get_variable(\"hidden2/weights\")\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.constant([0., 0., 3., 4., 30., 40., 300., 400.], shape=(4, 2))\n",
    "c = tf.clip_by_norm(x, clip_norm=10)\n",
    "c0 = tf.clip_by_norm(x, clip_norm=350, axes=0)\n",
    "c1 = tf.clip_by_norm(x, clip_norm=10, axes=1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    xv = x.eval()\n",
    "    cv = c.eval()\n",
    "    c0v = c0.eval()\n",
    "    c1v = c1.eval()\n",
    "\n",
    "print(xv)\n",
    "\n",
    "print(cv)\n",
    "\n",
    "print(np.linalg.norm(cv))\n",
    "\n",
    "print(c0v)\n",
    "\n",
    "print(np.linalg.norm(c0v, axis=0))\n",
    "\n",
    "print(c1v)\n",
    "\n",
    "print(np.linalg.norm(c1v, axis=1))\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "\n",
    "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\", collection=\"max_norm\"):\n",
    "    def max_norm(weights):\n",
    "        clip_weights = tf.assign(weights, tf.clip_by_norm(weights, clip_norm=threshold, axes=axes), name=name)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        return None # there is no regularization loss term\n",
    "    return max_norm\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    with arg_scope(\n",
    "            [fully_connected],\n",
    "            weights_regularizer=max_norm_regularizer(1.5)):\n",
    "        hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\")\n",
    "        hidden2 = fully_connected(hidden1, n_hidden2, scope=\"hidden2\")\n",
    "        logits = fully_connected(hidden2, n_outputs, activation_fn=None, scope=\"outputs\")\n",
    "\n",
    "clip_all_weights = tf.get_collection(\"max_norm\")\n",
    "       \n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    threshold = 1.0\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)\n",
    "    capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "                  for grad, var in grads_and_vars]\n",
    "    training_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(len(mnist.test.labels)//batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={is_training: True, X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={is_training: False, X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={is_training: False, X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"my_model_final.ckpt\")\n",
    "\n",
    "Dropout\n",
    "\n",
    "from tensorflow.contrib.layers import dropout\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "\n",
    "initial_learning_rate = 0.1\n",
    "decay_steps = 10000\n",
    "decay_rate = 1/10\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                           decay_steps, decay_rate)\n",
    "\n",
    "keep_prob = 0.5\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    with arg_scope(\n",
    "            [fully_connected],\n",
    "            activation_fn=tf.nn.elu,\n",
    "            weights_initializer=he_init):\n",
    "        X_drop = dropout(X, keep_prob, is_training=is_training)\n",
    "        hidden1 = fully_connected(X_drop, n_hidden1, scope=\"hidden1\")\n",
    "        hidden1_drop = dropout(hidden1, keep_prob, is_training=is_training)\n",
    "        hidden2 = fully_connected(hidden1_drop, n_hidden2, scope=\"hidden2\")\n",
    "        hidden2_drop = dropout(hidden2, keep_prob, is_training=is_training)\n",
    "        logits = fully_connected(hidden2_drop, n_outputs, activation_fn=None, scope=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)   \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(len(mnist.test.labels)//batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={is_training: True, X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={is_training: False, X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={is_training: False, X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"my_model_final.ckpt\")\n",
    "\n",
    "train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                               scope=\"hidden[2]|outputs\")\n",
    "\n",
    "training_op2 = optimizer.minimize(loss, var_list=train_vars)\n",
    "\n",
    "\n",
    "for i in tf.global_variables():\n",
    "    print(i.name)\n",
    "\n",
    "for i in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n",
    "    print(i.name)\n",
    "\n",
    "\n",
    "for i in train_vars:\n",
    "    print(i.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
